{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gskKSeycfnuO"
   },
   "source": [
    "## Análisis preliminar de datos\n",
    "\n",
    "A continuación se presenta un primer análisis basado en los datos del Censo INE 2011, publicados en la página web del Instituto.\n",
    "\n",
    "Además se utilizan varios conjuntos de datos disponibilizados en la web. Se realizó un pre-procesamiento para seleccionar variables relevantes, reduciendo el volúmen de datos y por tanto los requerimientos informáticos para su acceso.\n",
    "\n",
    "Como capas de información geográfica se se cuenta la capas de polígonos de departamento y de puntos de localidades del INE, identificando las capitales departamentales en esta última capa.\n",
    "\n",
    "Se incluye una matriz de distancias entre cada centro medio de población, calculada con la API Google Distance Matrix, que consta de distancias a pie por la red de caminería entre el centro medio de población de cada departamento, obteniendo una matriz con 342 valores ((19x19)-19).\n",
    "\n",
    "Se prefirió usar el centro medio de población, en detrimento del centroide o la capital departamental. El centro medio de población se calcula transfiriendo el conteo de habitantes del segmento censal al centroide de dicho segmento, aplicando luego la siguiente fórmula:\n",
    "\n",
    "$$\n",
    "\\overline{X}_w=\\frac{\\sum_{i=1}^{n}w_{i}X_{i}}{\\sum_{i=1}^{n} w_{i}}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\overline{Y}_w=\\frac{\\sum_{i=1}^{n}w_{i}Y_{i}}{\\sum_{i=1}^{n} w_{i}}\n",
    "$$\n",
    "\n",
    "dónde\n",
    "\n",
    "$$w = peso$$\n",
    "\n",
    "En este caso el \"peso\" (w) sería la población, en tanto que \"x\" e \"y\" son las coordenadas cartográficas de cada centroide.\n",
    "\n",
    "De esta forma se obtiene un par de coordenadas para cada departamento, que representa ese centro medio.\n",
    "\n",
    "El PBI departamental de toma de la información producida y publicada por OPP **(cita).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "eIjjW8B2nubF",
    "outputId": "75594d27-744f-40cd-b135-c9763a5d990e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import tabulate\n",
    "\n",
    "from shapely import wkt\n",
    "from shapely.geometry import shape, LineString, Point\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "3CLV2-_7-DX1",
    "outputId": "f03f427b-1cc0-4b43-b861-2983f08bb38d"
   },
   "outputs": [],
   "source": [
    "# carga datos\n",
    "\n",
    "# Datos censales\n",
    "censo = pd.read_csv('tablas/personas_censo_2011.zip', compression='zip', header=0, sep=',', quotechar='\"')\n",
    "\n",
    "# PBI departamental\n",
    "pbi = pd.read_csv('tablas/pbi_departamental.csv')\n",
    "\n",
    "# matriz de distancias\n",
    "md = pd.read_csv('tablas/df_distancias_centro_poblacion.csv')\n",
    "md.drop(['latlon_ori', 'latlon_des'], axis=1, inplace=True)\n",
    "\n",
    "# carga capa departamentos INE pg\n",
    "deptos = gpd.read_file('capas/ine_deptos.gpkg')\n",
    "\n",
    "# agrega centroides de departamentos\n",
    "deptos['centroide'] = deptos['geometry'].centroid\n",
    "\n",
    "# carga capa localidades INE pt\n",
    "localidad = gpd.read_file('capas/ine_localidades.gpkg')\n",
    "localidad.CODLOC = localidad.CODLOC.astype(int)\n",
    "\n",
    "# centro de población\n",
    "centro_pobl = gpd.read_file('capas/centro_poblacion.gpkg')\n",
    "\n",
    "# filtra capitales departamentales de las localidades INE\n",
    "capital = localidad[localidad.CAPITAL==True].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcula la población de cada departamento\n",
    "poblacion = censo[['DPTO', 'LOC']].groupby(by=['DPTO']).count()\n",
    "poblacion.rename(columns={'LOC': 'poblacion'}, inplace=True)\n",
    "\n",
    "# genera dataframe con datos por departamento\n",
    "datos_dpto = pd.merge(pbi, poblacion, on='DPTO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DPTO</th>\n",
       "      <th>NOMBRE</th>\n",
       "      <th>miles_de_pesos</th>\n",
       "      <th>porcentaje_pbi</th>\n",
       "      <th>poblacion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>ARTIGAS</td>\n",
       "      <td>14214980</td>\n",
       "      <td>1,5</td>\n",
       "      <td>73377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>CANELONES</td>\n",
       "      <td>86306492</td>\n",
       "      <td>9,3</td>\n",
       "      <td>520173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>CERRO LARGO</td>\n",
       "      <td>17048887</td>\n",
       "      <td>1,8</td>\n",
       "      <td>84698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>COLONIA</td>\n",
       "      <td>43788749</td>\n",
       "      <td>4,7</td>\n",
       "      <td>123203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>DURAZNO</td>\n",
       "      <td>13023942</td>\n",
       "      <td>1,4</td>\n",
       "      <td>57084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DPTO       NOMBRE  miles_de_pesos porcentaje_pbi  poblacion\n",
       "0     2      ARTIGAS        14214980            1,5      73377\n",
       "1     3    CANELONES        86306492            9,3     520173\n",
       "2     4  CERRO LARGO        17048887            1,8      84698\n",
       "3     5      COLONIA        43788749            4,7     123203\n",
       "4     6      DURAZNO        13023942            1,4      57084"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_dpto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis por departamentos**\n",
    "\n",
    "Dada la menor complejidad, se comienza por el análisis de los flujos entre departamentos.\n",
    "La base de personas del Censo INE 2011 se filtra según el siguiente criterio:\n",
    "- Variable \"PERMI07\" (lugar de residencia 5 años antes) con valores:\n",
    "    - 2 = en otra localidad o paraje de este departamento\n",
    "    - 3 = en otro departamento\n",
    "    \n",
    "Se excluyen habitantes de localidades rurales.\n",
    "\n",
    "La tabla resultante contiene un departamento de origen, uno de destino y una cantidad de personas que declaran haber vivido antes en el departamento de \"origen\", habiendo sido relevadas en el departamento de \"destino\" al momento de la aplicación del formulario censal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "p04HSl9Aa0tb",
    "outputId": "41934c34-bc0d-4ebc-9280-7e1591817bcd"
   },
   "outputs": [],
   "source": [
    "# define columnas para filtrar\n",
    "cols = ['DPTO', 'LOC', 'SECC', 'SEGM', 'PERMI07', 'PERMI07_1',\n",
    "        'PERMI07_2', 'PERMI07_3', 'PERMI07_4']\n",
    "\n",
    "# filtra\n",
    "p_migr = censo.loc[(censo.PERMI07 == 2) | (censo.PERMI07 == 3), cols]\n",
    "\n",
    "# cambia DPTO a tipo entero\n",
    "p_migr.DPTO = p_migr.DPTO.astype(int)\n",
    "\n",
    "# print(p_migr.shape[0])\n",
    "\n",
    "# Hay 26.449 residentes en localidades rurales\n",
    "# print(p_migr[( p_migr.LOC == 900 )].shape[0])\n",
    "\n",
    "# identifica depto de residencia anterior\n",
    "p_migr['depto_origen'] = p_migr.loc[:,('PERMI07_2')]\n",
    "p_migr.depto_origen.fillna(p_migr.DPTO, inplace=True)\n",
    "\n",
    "# convierte a string\n",
    "p_migr.loc[:,('depto_origen')] = p_migr.loc[:,('depto_origen')].astype(int)\n",
    "\n",
    "# renombra DPTO\n",
    "p_migr.rename(columns={'DPTO': 'depto_destino'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_migr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot de las capas de departamentos y localidades\n",
    "f, ax = plt.subplots(1,figsize=(8,6))\n",
    "\n",
    "# apaga ejes\n",
    "plt.axis('off')\n",
    "ax.get_yaxis().set_visible(False)\n",
    "ax.get_xaxis().set_visible(False)\n",
    "\n",
    "# deptos\n",
    "deptos.plot(color='w', edgecolor='silver',ax=ax)\n",
    "\n",
    "# localidades\n",
    "centro_pobl.plot(markersize=12, color = 'red', ax = ax, label='Centro medio de población')\n",
    "deptos.geometry.centroid.plot(markersize=4, color = 'silver', ax = ax, label='Centroide')\n",
    "capital.plot(markersize=7, color = 'orange', ax = ax, label='Capital departamental')\n",
    "\n",
    "# título\n",
    "# pl.title(\"Centroide, centro de población y capitales departamentales\", size=10)\n",
    "\n",
    "# referencias\n",
    "plt.legend(fontsize=9, frameon=False)\n",
    "\n",
    "metadatos_figs = {'Author': '''Guillermo D'Angelo''', 'Title': 'Mapa Centro de Población'}\n",
    "\n",
    "plt.savefig('mapas_graficas/centro_poblacion.pdf', bbox_inches='tight',\n",
    "            metadata = metadatos_figs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "BPugrUOVjHi6",
    "outputId": "e141867c-3e13-436b-ec09-9dad9666d008"
   },
   "outputs": [],
   "source": [
    "# genera un sólo dataframe solo para deptos\n",
    "flujos_deptos = p_migr[['depto_origen', 'depto_destino']]\n",
    "\n",
    "flujos_deptos = flujos_deptos[flujos_deptos.depto_origen != flujos_deptos.depto_destino]\n",
    "\n",
    "flujos_deptos['personas_mig'] = 1\n",
    "\n",
    "# agrupa y cuenta\n",
    "grupo = flujos_deptos.groupby(by=['depto_origen', 'depto_destino']).sum()\n",
    "\n",
    "print('Cantidad de díadas ', grupo.shape[0])\n",
    "\n",
    "grupo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grupo.reset_index().head().to_markdown(showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se presentan dichos datos en formato de matriz simétrica, refiriéndose a cada departamento con su código INE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 739
    },
    "colab_type": "code",
    "id": "qwpxiO0oh3T2",
    "outputId": "ce6fb4a5-c248-45cb-8b13-205797d672c4"
   },
   "outputs": [],
   "source": [
    "# genera tabla pivot con los flujos de departamento a departamento\n",
    "matrix = pd.pivot_table(flujos_deptos,\n",
    "                        index ='depto_origen',\n",
    "                        columns='depto_destino',\n",
    "                        fill_value=0,\n",
    "                        aggfunc=sum,\n",
    "                        margins=True,\n",
    "                        margins_name='Total')\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de nombres deptos para generar tabla en latex\n",
    "nomdep = ['Mvdeo.', 'Artigas', 'Can.', 'C. Largo', 'Colonia', 'Durazno',\n",
    "          'Flores', 'Florida', 'Lavalleja','Maldonado', 'Paysandú', 'R. Negro', 'Rivera',\n",
    "          'Rocha', 'Salto', 'San José', 'Soriano', 'Tacuarembó', 'T. y Tres']\n",
    "\n",
    "coddep = np.arange(1, 20, 1).tolist()\n",
    "\n",
    "# guarda tabla en latex\n",
    "# cabecera = datos_dpto.sort_values('DPTO')['NOMBRE'].to_list()\n",
    "# cabecera.append('Total')\n",
    "\n",
    "matrix_tex = matrix\n",
    "matrix_tex.rename(index=dict(zip(coddep, nomdep)), inplace=True)\n",
    "\n",
    "# setea ancho de columnas\n",
    "ancho = 'p{0.7cm}'\n",
    "colformato='l' + ancho * 20\n",
    "\n",
    "matrix_tex.to_latex(buf= \"tablas/matriz_orig_dest.tex\", bold_rows=False,\n",
    "                    column_format = colformato,\n",
    "                    caption= 'Matriz de movimientos entre departamentos (Censo INE 2011).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se contruye un conjunto de datos que contiene la siguiente información para cada díada de departamentos:\n",
    "- Los datos son los totales de personas que declaran haber vivido antes en el departamento de origen\n",
    "- La población total en origen y destino\n",
    "- El PBI en el departamento de destino y el logaritmo de dicho valor\n",
    "- La distancia entre cada centro medio de población y el logaritmo de dicho valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "ZmA9kwWBg5ng",
    "outputId": "400f9762-bb51-4d4c-f159-26ba4ba63a69"
   },
   "outputs": [],
   "source": [
    "# unimos todo en un dataframe de díadas\n",
    "df_agrupado = grupo.reset_index()\n",
    "\n",
    "# agrega codigo único\n",
    "df_agrupado.insert(0, 'cod', (df_agrupado['depto_origen'].astype(str)\n",
    "                   + df_agrupado['depto_destino'].astype(str).str.zfill(2)).astype(int))\n",
    "\n",
    "# recupera datos de otros dataframes\n",
    "cols = ['DPTO', 'NOMBRE', 'miles_de_pesos', 'poblacion']\n",
    "merge1 = pd.merge(df_agrupado, datos_dpto[cols],left_on='depto_origen', right_on='DPTO')\n",
    "merge2 = pd.merge(merge1, datos_dpto[cols], left_on='depto_destino', right_on='DPTO')\n",
    "merge3 = pd.merge(merge2, md[['cod', 'distancia']], on='cod')\n",
    "datos_diadas = merge3.drop(['DPTO_x', 'DPTO_y'], axis=1)\n",
    "\n",
    "# borra productos intermedios\n",
    "del(merge1, merge2, merge3)\n",
    "\n",
    "# renombra columnas\n",
    "dict_rename = {'NOMBRE_x': 'nom_depto_orig', 'NOMBRE_y': 'nom_depto_des',\n",
    "               'miles_de_pesos_x': 'pbi_origen', 'miles_de_pesos_y': 'pbi_destino',\n",
    "               'distancia': 'dist', 'poblacion_x': 'pob_origen',\n",
    "               'poblacion_y': 'pob_destino'}\n",
    "\n",
    "datos_diadas.rename(columns=dict_rename, inplace=True)\n",
    "\n",
    "# incorporamos el logaritmo del PBI departamental en destino\n",
    "datos_diadas = datos_diadas.assign(log_pbi_destino = lambda x: np.log(x['pbi_destino']))\n",
    "\n",
    "# incorporamos el logaritmo de las distancias entre díadas\n",
    "datos_diadas = datos_diadas.assign(log_dist = lambda x: np.log(x['dist']))\n",
    "\n",
    "# reordena variables\n",
    "order = ['cod', 'depto_origen', 'nom_depto_orig',\n",
    "         'depto_destino', 'nom_depto_des', 'personas_mig',\n",
    "         'pbi_origen', 'pob_origen', 'pbi_destino', 'pob_destino',\n",
    "         'dist', 'log_pbi_destino', 'log_dist']\n",
    "\n",
    "datos_diadas = datos_diadas[order]\n",
    "datos_diadas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guarda\n",
    "datos_diadas.to_csv('tablas/datos_diadas_2011.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define función para hacer lineas a partir de códigos origen-destino y una geografía conocida\n",
    "# fuente: https://github.com/danlewis85/UCL_CASA_Urban_Simulation/blob/master/Unconstrained%20Spatial%20Interaction%20Models.ipynb\n",
    "\n",
    "def _odline(orig, dest, geo, zonecode):\n",
    "    return LineString([deptos[geo[zonecode] == orig].centroid.values[0], geo[geo[zonecode] == dest].centroid.values[0]])\n",
    "\n",
    "# Makes a geodataframe of flows\n",
    "def odflow(flowdata, origin, destination, flow_value, geo, zonecode):\n",
    "    # First make all the lines\n",
    "    lines = flowdata.apply(lambda x: _odline(x[origin], x[destination], geo, zonecode), axis=1)\n",
    "    # Now get the series of flow values\n",
    "    flows = flowdata[[flow_value, origin, destination]]\n",
    "    # Now return a geodataframe\n",
    "    return gpd.GeoDataFrame(flows, geometry=lines, crs = geo.crs)\n",
    "\n",
    "# aplica función\n",
    "flows = odflow(datos_diadas,'depto_origen', 'depto_destino', 'personas_mig', deptos,'cod_ine')\n",
    "\n",
    "# guarda como geopaquete\n",
    "flows.to_file(\"capas/mig_recientes_2011.gpkg\", layer='flujos', driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot de flujos\n",
    "f, ax = plt.subplots(1,figsize=(8,6))\n",
    "\n",
    "# apaga ejes\n",
    "plt.axis('off')\n",
    "ax.get_yaxis().set_visible(False)\n",
    "ax.get_xaxis().set_visible(False)\n",
    "\n",
    "# capa de departamentos\n",
    "deptos.plot(color='w', edgecolor='silver' ,ax=ax)\n",
    "\n",
    "# máximo de flujos para escalado\n",
    "maxflow = float(max(flows['personas_mig']))\n",
    "\n",
    "# título\n",
    "# pl.title(\"Representación gráfica de las migraciones recientes\", size=10)\n",
    "\n",
    "# plotea flows, calcula ancha de línea con una función sobre 'personas_mig'\n",
    "flows.plot(linewidth = flows.apply(lambda x: (x['personas_mig']/maxflow)*10, axis=1),\n",
    "           color = 'red',ax=ax, label='Flujos entre departamentos', alpha=0.5)\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# referencias\n",
    "plt.legend(fontsize=9)\n",
    "\n",
    "plt.savefig('mapas_graficas/links_depto.pdf', bbox_inches='tight',\n",
    "            metadata = metadatos_figs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "colab_type": "code",
    "id": "dUb6ZM9duK49",
    "outputId": "8f6c58ef-1cea-41b4-f125-557bc49ba26a"
   },
   "outputs": [],
   "source": [
    "# Gráfico de flujos contra distancia\n",
    "f, ax = plt.subplots(1, figsize=(10,6))\n",
    "\n",
    "# plot de los puntos\n",
    "ax.scatter(datos_diadas['dist'], datos_diadas['personas_mig'], marker='.', color='k')\n",
    "\n",
    "# crea la linea roja\n",
    "xvals = np.geomspace(0.0055, datos_diadas['dist'].max(), 1000)\n",
    "yvals = np.power(xvals,-2.0)\n",
    "\n",
    "# la agrega al plot\n",
    "ax.plot(xvals, yvals, color='r')\n",
    "\n",
    "# hide spines\n",
    "[ax.spines[i].set_visible(False) for i in ['right', 'top', 'left', 'bottom']]\n",
    "\n",
    "# etiquetas\n",
    "ax.set_ylabel(\"Total de flujos\")\n",
    "ax.set_xlabel(\"Distancia (m)\")\n",
    "\n",
    "# se ven dos outlyers claramente, deben ser Mvdeo y Canelones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mismo gráfico sin Mvdeo.\n",
    "sin_mvo = datos_diadas[(datos_diadas['depto_origen'] > 1) & (datos_diadas['depto_destino'] > 1)]\n",
    "\n",
    "# Gráfico de flujos contra distancia\n",
    "f, ax = plt.subplots(1, figsize=(10,6))\n",
    "\n",
    "# plot de los puntos\n",
    "ax.scatter(sin_mvo['dist'], sin_mvo['personas_mig'], marker='.', color='k')\n",
    "\n",
    "# crea la linea roja\n",
    "xvals = np.geomspace(0.025, sin_mvo['dist'].max(), 100)\n",
    "yvals = np.power(xvals,-2.0)\n",
    "\n",
    "# la agrega al plot\n",
    "ax.plot(xvals, yvals, color='r')\n",
    "\n",
    "# Etiquetas\n",
    "ax.set_ylabel(\"Total de flujos\")\n",
    "ax.set_xlabel(\"Distancia (m)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gráfico de flujos contra población en el origen\n",
    "f, ax = plt.subplots(1, figsize=(10,6))\n",
    "\n",
    "# Plot data points\n",
    "ax.scatter(datos_diadas['pob_origen'], datos_diadas['personas_mig'], marker='.', color='k')\n",
    "\n",
    "# now work out the function y = x - basic linear slope with 0 origin.\n",
    "xvals = np.linspace(datos_diadas['pob_origen'].min(), datos_diadas['pob_origen'].max(), 100)\n",
    "yvals = np.power(xvals, 1.0)\n",
    "\n",
    "# Now add function line to plot\n",
    "ax.plot(xvals,yvals,color='r')\n",
    "\n",
    "# need to set the ylim to the domain of the origin pops, so we see the full line.\n",
    "ax.set_ylim(-5000, datos_diadas['pob_origen'].max()*1.05)\n",
    "\n",
    "# Add some labels\n",
    "ax.set_ylabel(\"Total de flujos\")\n",
    "ax.set_xlabel(\"Población en origen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot de PBI en destino contra personas migrantes\n",
    "f, ax = plt.subplots(1,figsize=(8,8))\n",
    "\n",
    "# Plot data points\n",
    "ax.scatter(datos_diadas['pbi_destino'], datos_diadas['personas_mig'], marker='.',color='k')\n",
    "\n",
    "# now work out the function y = x - basic linear slope with 0 origin.\n",
    "xvals = np.linspace(datos_diadas['pbi_destino'].min(), datos_diadas['pbi_destino'].max(), 100)\n",
    "yvals = np.power(xvals,1.0)\n",
    "\n",
    "# Now add function line to plot\n",
    "ax.plot(xvals,yvals,color='r')\n",
    "\n",
    "# need to set the ylim to the domain of the origin pops, so we see the full line.\n",
    "ax.set_ylim(-1000, datos_diadas['personas_mig'].max()*1.05)\n",
    "\n",
    "# Add some labels\n",
    "ax.set_ylabel(\"Total de flujos\")\n",
    "ax.set_xlabel(\"PBI en destino\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo restringido en origen\n",
    "\n",
    "1 $$T_{ij} = A_{i}O_{i}W_{j}^{\\alpha}d_{ij}^{-\\beta}$$\n",
    "\n",
    "dónde\n",
    "\n",
    "2 $$O_{i} = \\sum_{j}T_{ij}$$\n",
    "\n",
    "\n",
    "3 $$A_{i} = \\frac{1}{\\sum_{j}W_{j}^{\\alpha}d_{ij}^{-\\beta}}$$\n",
    "\n",
    "\n",
    "\n",
    "En el modelo restringido en origen $O_{i}$ no tiene parámetro dado que representa valores conocidos. $A_{i}$ es un factor de balance que refiere a cada origen $i$. Más específicamente $A_{i}$ permite que la suma de los valores estimados sea igual al total conocido $O_{i}$\n",
    "\n",
    "El modelo es re-especificado como un modelo de regresión de Poisson. Se asume una vinculación\n",
    "\n",
    "**We set about re-specifying the Production-Constrained model as a Poisson regression model in a similar way to how we did before. We need to take logs of the right-hand side of equation and assume that these are logarithmially linked to the Poisson distributed mean ($\\lambda_{ij}$) of the $T_{ij}$ variable. Equation 1 (above) then becomes:**\n",
    "\n",
    "\n",
    "4   $$ \\lambda_{ij} = \\exp( \\mu_{i} + \\alpha \\ln W_{j} - \\beta \\ln d_{ij} )$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "d6_Tr79nUp59",
    "outputId": "222cb649-971b-4a52-b2b3-6887c55bbeb9"
   },
   "outputs": [],
   "source": [
    "# el depto_origen se cambio a tipo texto para que no sea tomada como variable numérica por la regresión\n",
    "#  datos_diadas['depto_origen'] = datos_diadas.depto_origen.astype(str)\n",
    "\n",
    "# respalda el objeto para usarlo en el otro modelo\n",
    "dd=datos_diadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_diadas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados de la aplicación del modelo restringido en origen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "colab_type": "code",
    "id": "fnWKSI3s3NTZ",
    "outputId": "4247178e-8d02-4550-bd6a-97455d97d114"
   },
   "outputs": [],
   "source": [
    "# Here we specify a model with no intercept (given by the -1 in the formula)\n",
    "# In practice this means that all AiOis are estimated against an intercept of zero.\n",
    "# Including the interval would mean setting the first borough in OrigNewCode to the intercept\n",
    "# and interpreting all other categories in relation to that, which is less useful but would still work.\n",
    "formula = \"personas_mig ~ nom_depto_orig + log_pbi_destino + log_dist -1\"\n",
    "prodSim = smf.glm(formula=formula, data = datos_diadas, family = sm.families.Poisson()).fit()\n",
    "prodSim.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prodSim_latex = prodSim.summary().as_latex()\n",
    "f = open(\"tablas/prodSim.tex\", \"w\")\n",
    "f.write(prodSim_latex)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbNmxtmKOOhp"
   },
   "source": [
    "De los resultados se desprende un parámetro $\\alpha$ relacionado a la actractividad del destino de 0,8527.\n",
    "\n",
    "El parámetro $\\beta$ relativo al decaimiento por la distancia es de -0,7830\n",
    "\n",
    "El coeficiente para cada origen es el valor registrado $A_{i}O_{i}$ para ese origen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimaciones del modelo restringido en origen\n",
    "\n",
    "Los parámetros calculados se insertan en la ecuación nro. 4.\n",
    "\n",
    "$$ \\lambda_{ij} = \\exp( \\mu_{i} + 0,8527 ln W_{j}  + 0,7830 ln d_{ij} )$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some Oi and Dj columns in the dataframe and store row and column totals in them:\n",
    "# First get the origin sums and rename the column created\n",
    "O_i = datos_diadas.groupby('depto_origen')['personas_mig'].sum().to_frame()\n",
    "O_i.rename(columns = {'personas_mig':'O_i'}, inplace=True)\n",
    "\n",
    "# Now get the destination sums\n",
    "D_j = datos_diadas.groupby('depto_destino')['personas_mig'].sum().to_frame()\n",
    "D_j.rename(columns = {'personas_mig':'D_j'}, inplace=True)\n",
    "\n",
    "# Merge in O_i\n",
    "datos_diadas = datos_diadas.merge(O_i,left_on='depto_origen', right_index=True)\n",
    "\n",
    "# Merge in D_j\n",
    "datos_diadas = datos_diadas.merge(D_j,left_on='depto_destino', right_index=True)\n",
    "\n",
    "datos_diadas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recupera los parámetros del modelo\n",
    "mu_i = prodSim.params.to_frame()\n",
    "\n",
    "# elimina caractérres no numéricos para poder mergear\n",
    "mu_i.rename(index = dict(zip(mu_i.index[0:-2].values, mu_i.index[0:-2].str.replace(r'[^ ABCDEFGHIJKLMNÑOPQRSTUVWXYZ]','').values)),\n",
    "            inplace=True)\n",
    "\n",
    "# renombre columna\n",
    "mu_i.rename(columns = {0:'mu_i'}, inplace=True)\n",
    "\n",
    "# merge\n",
    "datos_diadas = datos_diadas.merge(mu_i, left_on='nom_depto_orig', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guarda parámetros estimados en objetos\n",
    "alpha = prodSim.params[19]\n",
    "beta  = prodSim.params[20]\n",
    "\n",
    "print(\"alfa (log PBI destino)= \" + str(alpha))\n",
    "print(\"beta (log distancia)= \" + str(beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genera estimación redondeada\n",
    "# esta es la estimación del modelo de la ecuación 4, imputando los parámetro alfa y beta calculados\n",
    "datos_diadas['prodsimest'] = np.round(np.exp(datos_diadas['mu_i']\n",
    "                                             + alpha * datos_diadas['log_pbi_destino']\n",
    "                                             + beta * datos_diadas['log_dist']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matriz de flujos estimada por el modelo\n",
    "datos_diadas['depto_origen'] = datos_diadas.depto_origen.astype(int)\n",
    "\n",
    "matrix_prodsim = pd.pivot_table(datos_diadas,\n",
    "                                values='prodsimest',\n",
    "                                index ='depto_origen',\n",
    "                                columns='depto_destino',\n",
    "                                fill_value=0,\n",
    "                                aggfunc=sum,\n",
    "                                margins=True,\n",
    "                                margins_name='Total')\n",
    "\n",
    "matrix_prodsim.Total = matrix_prodsim.Total.astype(int)\n",
    "\n",
    "matrix_prodsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_prodsim_tex = matrix_prodsim.rename(index=dict(zip(coddep, nomdep)))\n",
    "\n",
    "matrix_prodsim_tex.to_latex(buf= \"tablas/prodsim_matriz_orig_dest.tex\", bold_rows=False,\n",
    "                            column_format = colformato,  float_format=\"%.2f\",\n",
    "                            caption= 'Matriz de movimientos entre departamentos estimada mediante SIM restringido en origen.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede identificar la coincidencia de la matriz de los datos originales con la de los datos estimados en la columna de origen $O_{i}$, con leves diferencias producto del redondeo.\n",
    "\n",
    "$$\\sum_{j}T_{ij} = \\sum_{j}\\lambda_{ij} = O_{i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bondad de ajuste\n",
    "\n",
    "# define una función para calcular el R cuadrado\n",
    "def calcR2(obs, est):\n",
    "    return np.power(np.corrcoef(obs,est),2.0)[0][1]\n",
    "\n",
    "# define una función para calcula el error medio cuadrático\n",
    "def calcRMSE(obs,est):\n",
    "    return np.sqrt((np.power((obs - est),2.0)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('**Bondad de ajuste del modelo restringido en origen**')\n",
    "\n",
    "printmd(\"$R²$ = \" + round(calcR2(datos_diadas['personas_mig'],datos_diadas['prodsimest']), 4).astype(str))\n",
    "\n",
    "printmd(\"RMSE = \" + round(calcRMSE(datos_diadas['personas_mig'],datos_diadas['prodsimest']), 4).astype(str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de doble restricción\n",
    "\n",
    "5    $$ T_{ij} = A_{i}O_{i}B_{i}D_{j}d_{ij}^{-\\beta }$$\n",
    "\n",
    "dónde\n",
    "\n",
    "6 $$O_{i} = \\sum_{j}T_{ij}$$\n",
    "\n",
    "7 $$D_{j} = \\sum_{i}T_{ij}$$\n",
    "\n",
    "8 $$A_{i} = \\frac{1}{\\sum_{j}B_{j}D_{j}d_{ij}^{-\\beta}}$$\n",
    "\n",
    "9 $$B_{j} = \\frac{1}{\\sum_{j}A_{i}O_{j}d_{ij}^{-\\beta}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dificultad es que $A_{i}$ depende de $B_{j}$ y viceversa. Pero se puede arrivar a un valor para ambos factores fijando el valor de $B$ inicialmente como 1, y luego iterando refinando el valor de cada uno hasta que sea estable (converjan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de restricción doble\n",
    "\n",
    "# recupera los datos del respaldo\n",
    "datos_diadas = dd\n",
    "# formula = \"personas_mig ~ nom_depto_orig + nom_depto_des + log_pbi_destino + log_dist -1\"\n",
    "\n",
    "# siguiendo a Dennett y la guía de R, cambiamos la fórmula, borrando el \"-1\" al final\n",
    "# es decir que se mantiene la intersección\n",
    "\n",
    "# The code below has changed a litte from the singly constrained models I have removed the ‘-1’\n",
    "# which means that an intercept will appear in the model again. This is not because I want an\n",
    "# intercept as it makes the origin and destination coefficients harder to interpret - reference categories\n",
    "# zones will appear and the coefficients will need to be compared with the intercept - rather the ‘-1’ cheat\n",
    "# for removing the intercept only works with one factor level - here we have two (origins and destinations).\n",
    "# For full details and an explanation for alternative ways for dealing with this, please visit\n",
    "# here - https://stats.stackexchange.com/questions/215779/removing-intercept-from-glm-for-multiple-factorial-predictors-only-works-for-fir - for ease, here we will just continue with the intercept.\n",
    "\n",
    "formula = \"personas_mig ~ nom_depto_orig + nom_depto_des + log_pbi_destino + log_dist\"\n",
    "\n",
    "doubSim = smf.glm(formula=formula, data = datos_diadas, family = sm.families.Poisson()).fit()\n",
    "doubSim.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guarda DF para enviar a Eugenia\n",
    "datos_diadas.head()\n",
    "datos_diadas.to_csv('tablas/datos_diadas.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubSim_latex = doubSim.summary().as_latex()\n",
    "f = open(\"tablas/doubSim.tex\", \"w\")\n",
    "f.write(doubSim_latex)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recupera los valores estimados\n",
    "datos_diadas['doubsim_ajustado'] = np.round(doubSim.predict())\n",
    "\n",
    "datos_diadas['depto_origen'] = datos_diadas.depto_origen.astype(int)\n",
    "\n",
    "# matriz de los valores estimados\n",
    "matrix_doubsim = pd.pivot_table(datos_diadas,\n",
    "                                values='doubsim_ajustado',\n",
    "                                index ='depto_origen',\n",
    "                                columns='depto_destino',\n",
    "                                fill_value=0,\n",
    "                                aggfunc=sum,\n",
    "                                margins=True,\n",
    "                                margins_name='Total')\n",
    "\n",
    "matrix_doubsim.Total = matrix_doubsim.Total.astype(int)\n",
    "\n",
    "matrix_doubsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_doubsim_tex = matrix_doubsim.rename(index=dict(zip(coddep, nomdep)))\n",
    "\n",
    "matrix_doubsim_tex.to_latex(buf = \"tablas/doubsim_matriz_orig_dest.tex\",\n",
    "                            bold_rows = False,\n",
    "                            column_format = colformato,\n",
    "                            float_format =\"%.2f\",\n",
    "                            caption = 'Matriz de movimientos entre departamentos estimada mediante SIM de doble restricción.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar la igualdad de los valores originales $O_{i}$ y $D_{j}$, pero al igual que en el modelo de restricción en origen se producen leves direrencias que atrubuimos al redondeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('**Bondad de ajuste del modelo de doble restricción**')\n",
    "\n",
    "printmd(\"$R²$ = \" + round(calcR2(datos_diadas['personas_mig'],  datos_diadas['doubsim_ajustado']), 4).astype(str))\n",
    "\n",
    "printmd(\"RMSE = \" + round(calcRMSE(datos_diadas['personas_mig'],datos_diadas['doubsim_ajustado']), 4).astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El coeficiente de determinación $R²$ mejora en comparación con resultante del modelo restringido en origen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a departure from Dennett, I've rewritten the algorithm as a function, which can then be called subject to the required parameters. In order for it to work it requires the following things:\n",
    "\n",
    "    pd - a pandas dataframe of origin-destination pairwise flows and associated data.\n",
    "    orig_field - the name of the dataframe field in pd that uniquely labels origin zones.\n",
    "    dest_field - the name of the dataframe field in pd that uniquely labels destination zones.\n",
    "    Oi_field - the name of the dataframe field that stores total flows from a given origin $i$\n",
    "    Dj_field - the name of the dataframe field that stores total flows to a given destination $j$\n",
    "    cij_field - the name of the dataframe field that stores the pairwise cost (e.g. distance) between $i$ and $j$\n",
    "    beta - a constant for the beta parameter you wish to use in the model\n",
    "    cost_function - a string representing the cost function, either 'power' or 'exponential'\n",
    "    Ainame - What you want to call the new field in pd that will hold $A&lt;/em&gt;{i}$ values, defaults to \"Ai_new\"\n",
    "    Bjname - What you want to call the new field in pd that will hold $B&lt;/em&gt;{j}$ values, defaults to \"Bj_new\"\n",
    "    converge - A threshold value at which a model can be said to have converged, the default of 0.001 seems to work fine.\n",
    "\n",
    "NB Remember that we calculated $O_{i}$ and $D_{j}$ earlier, they are simply the total flows by either origin or destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_diadas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the entropy maximising approach for a known beta.\n",
    "# Plug in the required values in this function to solve.\n",
    "\n",
    "def balance_doubly_constrained(pd, orig_field, dest_field,\n",
    "                               Oi_field, Dj_field, cij_field, beta, \n",
    "                               cost_function,\n",
    "                               Ai_name = \"Ai_new\", Bj_name = \"Bj_new\", converge=0.001):\n",
    "    # Define some variables\n",
    "    Oi = pd[[orig_field, Oi_field]]\n",
    "    Dj = pd[[dest_field,Dj_field]]\n",
    "    \n",
    "    if cost_function.lower() in ['power','pow']:\n",
    "        beta_cij = np.exp(beta * np.log(pd[cij_field]))\n",
    "    elif cost_function.lower() in ['exponential','exp']:\n",
    "        beta_cij = np.exp(beta * pd[cij_field])\n",
    "    else:\n",
    "        return \"Cost function not specified properly, use 'exp' or 'pow'\"\n",
    "    \n",
    "    # Create some helper variables\n",
    "    cnvg = 1\n",
    "    iteration = 0\n",
    "    \n",
    "    # Now iteratively rebalance the Ai and Bj terms until convergence\n",
    "    while cnvg > converge:\n",
    "        if iteration == 0:\n",
    "            # This first condition sets starting values for Ai and Bj\n",
    "            # NB sets starting value of Ai assuming Bj is a vector of 1s.\n",
    "            # We've already established beta_cij with the appropriate cost function, so...\n",
    "            Oi = Oi.assign(Ai = Dj[Dj_field] * beta_cij)\n",
    "            # Aggregate Ai and take inverse\n",
    "            Ai = 1.0/Oi.groupby(orig_field)['Ai'].sum().to_frame()\n",
    "            # Merge new Ais \n",
    "            Oi = Oi.merge(Ai,left_on = orig_field, right_index = True, suffixes = ('','_old'))\n",
    "            # Drop the temporary Ai field we created, leaving Ai_old\n",
    "            Oi.drop('Ai', axis=1, inplace=True)\n",
    "            \n",
    "            # Now set up Bjs using starting values of Ai\n",
    "            Dj = Dj.assign(Bj = Oi['Ai_old'] * Oi[Oi_field] * beta_cij)\n",
    "            # Aggregate Bj and take inverse\n",
    "            Bj = 1.0/Dj.groupby(dest_field)['Bj'].sum().to_frame()\n",
    "            # Merge new Bjs\n",
    "            Dj = Dj.merge(Bj,left_on = dest_field, right_index = True, suffixes = ('','_old'))\n",
    "            # Drop the temporary Bj field we created, leaving Bj_old\n",
    "            Dj.drop('Bj', axis=1, inplace=True)\n",
    "            \n",
    "            # Increment loop\n",
    "            iteration += 1\n",
    "        else:\n",
    "            # This bit is the iterated bit of the loop which refines the values of Ai and Bj\n",
    "            # First Ai\n",
    "            Oi['Ai'] = Dj['Bj_old'] * Dj[Dj_field] * beta_cij\n",
    "            # Aggregate Ai and take inverse\n",
    "            Ai = 1.0/Oi.groupby(orig_field)['Ai'].sum().to_frame()\n",
    "            # Drop temporary Ai\n",
    "            Oi.drop('Ai', axis=1, inplace=True)\n",
    "            # Merge new Ais \n",
    "            Oi = Oi.merge(Ai,left_on = orig_field, right_index = True)\n",
    "            # Calculate the difference between old and new Ais\n",
    "            Oi['diff'] = np.absolute((Oi['Ai_old'] - Oi['Ai'])/Oi['Ai_old'])\n",
    "            # Set new Ais to Ai_old\n",
    "            Oi['Ai_old'] = Oi['Ai']\n",
    "            # Drop the temporary Ai field we created, leaving Ai_old\n",
    "            Oi.drop('Ai', axis=1, inplace=True)\n",
    "            \n",
    "            # Then Bj\n",
    "            Dj['Bj'] = Oi['Ai_old'] * Oi[Oi_field] * beta_cij\n",
    "            # Aggregate Bj and take inverse\n",
    "            Bj = 1.0/Dj.groupby(dest_field)['Bj'].sum().to_frame()\n",
    "            # Drop temporary Bj\n",
    "            Dj.drop('Bj', axis=1, inplace=True)\n",
    "            # Merge new Bjs\n",
    "            Dj = Dj.merge(Bj,left_on = dest_field, right_index = True)\n",
    "            # Calculate the difference between old and new Bjs\n",
    "            Dj['diff'] = np.absolute((Dj['Bj_old'] - Dj['Bj'])/Dj['Bj_old'])\n",
    "            # Set new Bjs to Bj_old\n",
    "            Dj['Bj_old'] = Dj['Bj']\n",
    "            # Drop the temporary Bj field we created, leaving Bj_old\n",
    "            Dj.drop('Bj', axis=1, inplace=True)\n",
    "            \n",
    "            # Assign higher sum difference from Ai or Bj to cnvg\n",
    "            cnvg = np.maximum(Oi['diff'].sum(),Dj['diff'].sum())\n",
    "            \n",
    "            # Print and increment loop\n",
    "            print(\"Iteration:\", iteration)\n",
    "            iteration += 1\n",
    "\n",
    "    # When the while loop finishes add the computed Ai_old and Bj_old to the dataframe and return\n",
    "    pd[Ai_name] = Oi['Ai_old']\n",
    "    pd[Bj_name] = Dj['Bj_old']\n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some Oi and Dj columns in the dataframe and store row and column totals in them:\n",
    "# First get the origin sums and rename the column created\n",
    "O_i = datos_diadas.groupby('depto_origen')['personas_mig'].sum().to_frame()\n",
    "O_i.rename(columns = {'personas_mig':'O_i'}, inplace=True)\n",
    "\n",
    "# Now get the destination sums\n",
    "D_j = datos_diadas.groupby('depto_destino')['personas_mig'].sum().to_frame()\n",
    "D_j.rename(columns = {'personas_mig':'D_j'}, inplace=True)\n",
    "\n",
    "# Merge in O_i\n",
    "datos_diadas = datos_diadas.merge(O_i,left_on='depto_origen', right_index=True)\n",
    "\n",
    "# Merge in D_j\n",
    "datos_diadas = datos_diadas.merge(D_j,left_on='depto_destino', right_index=True)\n",
    "\n",
    "datos_diadas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recupera el beta del logaritmo de la distancia del modelo anterior\n",
    "beta = doubSim.params[-1]\n",
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recupera factores de balance\n",
    "datos_diadas = balance_doubly_constrained(datos_diadas, 'nom_depto_orig', 'nom_depto_des', 'O_i', 'D_j', 'log_dist', beta, \n",
    "                                          'pow', Ai_name = \"Ai_new\", Bj_name = \"Bj_new\", converge=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_diadas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now predict the model again using the new Ai and Dj fields.\n",
    "datos_diadas['SIM_est_pow'] = np.round(datos_diadas['O_i'] * datos_diadas['Ai_new'] * datos_diadas['D_j'] * datos_diadas['Bj_new'] * \n",
    "                                       np.exp(np.log(datos_diadas['log_dist'])*beta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the matrix\n",
    "pd.pivot_table(datos_diadas, values='SIM_est_pow', index ='depto_origen',\n",
    "               columns='depto_destino', fill_value=0, aggfunc=sum, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd('**Bondad de ajuste del modelo de doble restricción**')\n",
    "\n",
    "printmd(\"$R²$ = \" + round(calcR2(datos_diadas['personas_mig'],  datos_diadas['doubsim_ajustado']), 4).astype(str))\n",
    "\n",
    "printmd(\"RMSE = \" + round(calcRMSE(datos_diadas['personas_mig'],datos_diadas['doubsim_ajustado']), 4).astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jugar con modelación de distance decay\n",
    "# jugar con distancia en lugar de logaritmo\n",
    "# probar con algún modelo kitchen sink\n",
    "# graficas y algún mapa\n",
    "#!jupyter nbconvert --to markdown --no-input 02-metodologia.ipynb \n",
    "\n",
    "# probar paquete SPInt taylor oshan\n",
    "\n",
    "# ATENCIÓN!\n",
    "# Another thing to note is that the example we used here had quite neat data. You will almost certainly run into problems if you have sparse data or predictors with 0s in them. If this happens, then you might need to either drop some rows in your data (if populated with 0s) or substitute 0s for very small numbers, much less than 1, but greater than 0 (this is because you can’t take the log of 0). Taylor Oshan's SpInt implementation in Python uses a special Poisson regression approach that better handles sparse data structures.\n",
    "\n",
    "\n",
    "# modelar con binomial negativa\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPPDYSSpI6HjvyquGBWo+hx",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "0.0_data_wrangling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
